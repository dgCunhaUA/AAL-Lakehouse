{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import *\n",
    "from video_rppg_extraction import VideoRppgExtraction\n",
    "from signal_processer import SignalProcessor\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, max\n",
    "import pyspark\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from minio import Minio\n",
    "import time\n",
    "import subprocess\n",
    "from settings import RAW_BUCKET_NAME, S3A_ENDPOINT, LANDING_BUCKET_NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.connection.maximum\n",
      "Ivy Default Cache set to: /Users/cunha/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/cunha/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dbb061ce-2151-460d-854c-4941d1262cc4;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/cunha/opt/anaconda3/envs/delta-lake-env/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 277ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dbb061ce-2151-460d-854c-4941d1262cc4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/31 15:20:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ACCESS_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/x1/pg5bb60j23l730md3mg_pp4r0000gn/T/ipykernel_4796/1653747282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_spark_with_delta_pip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.hadoop:hadoop-aws:3.3.1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"localhost:9000\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mACCESS_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSECRET_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ACCESS_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop.fs.s3a.S3AFileSystem, org.apache.hadoop.hadoop-aws\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"delta\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3A_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.fas.upload\", True) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.multipart.size\", 104857608) \\\n",
    "    .config(\"fs.s3a.connection.maximum\", 100) \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\")  \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.databricks.delta.properties.defaults.enableChangeDataFeed\", \"true\") \\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder, [\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\", \"org.apache.hadoop:hadoop-aws:3.3.1\"]).getOrCreate()\n",
    "\n",
    "client = Minio(\"localhost:9000\", access_key=\"minioadmin\", secret_key=\"minioadmin\", secure=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initalization and Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/31 15:21:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "23/05/31 15:22:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<delta.tables.DeltaTable at 0x7ff409323f50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize necessary classes\n",
    "video_extractor = VideoRppgExtraction()\n",
    "signal_processer = SignalProcessor()\n",
    "\n",
    "# Define schema for DataFrame\n",
    "filtered_bvp_schema = StructType(\n",
    "    [\n",
    "        StructField(\"filtered_bvp\", ArrayType(DoubleType())),\n",
    "        StructField(\"fps\", DoubleType()),\n",
    "        StructField(\"start_timestamp\", TimestampType()),\n",
    "        StructField(\"end_timestamp\", TimestampType()),\n",
    "        StructField(\"subject_uuid\", StringType()),\n",
    "        StructField(\"video\", StringType())\n",
    "    ]\n",
    ")\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://trusted-zone/trusted_filtered_bvp_signal\")\n",
    "    .tableName(\"trusted_filtered_bvp_signal\")\n",
    "    .addColumns(filtered_bvp_schema)\n",
    "    .execute()\n",
    ") \n",
    "\n",
    "\n",
    "#spark.sql(\"ALTER TABLE default.filtered_bvp_signal SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "\n",
    "trusted_heartpy_measures_schema = StructType([\n",
    "    StructField(\"bpm\", DoubleType(), True),\n",
    "    StructField(\"breathingrate\", DoubleType(), True),\n",
    "    StructField(\"end_timestamp\", TimestampType(), True),\n",
    "    StructField(\"hr_mad\", DoubleType(), True),\n",
    "    StructField(\"ibi\", DoubleType(), True),\n",
    "    StructField(\"pnn20\", DoubleType(), True),\n",
    "    StructField(\"pnn50\", DoubleType(), True),\n",
    "    StructField(\"rmssd\", DoubleType(), True),\n",
    "    StructField(\"s\", DoubleType(), True),\n",
    "    StructField(\"sd1\", DoubleType(), True),\n",
    "    StructField(\"sd1/sd2\", DoubleType(), True),\n",
    "    StructField(\"sd2\", DoubleType(), True),\n",
    "    StructField(\"sdnn\", DoubleType(), True),\n",
    "    StructField(\"sdsd\", DoubleType(), True),\n",
    "    StructField(\"start_timestamp\", TimestampType(), True),\n",
    "    StructField(\"subject_uuid\", StringType(), True),\n",
    "    StructField(\"video\", StringType(), True)\n",
    "])\n",
    "(DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://trusted-zone/trusted_heartpy_measures\")\n",
    "    .tableName(\"trusted_heartpy_measures\")\n",
    "    .addColumns(trusted_heartpy_measures_schema)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "trusted_working_data_schema = StructType([\n",
    "    StructField(\"RR_diff\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"RR_list\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"RR_sqdiff\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"breathing_frq\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"breathing_psd\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"breathing_signal\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"end_timestamp\", TimestampType(), True),\n",
    "    StructField(\"hr\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"nn20\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"nn50\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"removed_beats_y\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"rolling_mean\", ArrayType(DoubleType(), True), True),\n",
    "    StructField(\"rrsd\", DoubleType(), True),\n",
    "    StructField(\"sample_rate\", DoubleType(), True),\n",
    "    StructField(\"start_timestamp\", TimestampType(), True),\n",
    "    StructField(\"subject_uuid\", StringType(), True),\n",
    "    StructField(\"video\", StringType(), True)\n",
    "])\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://trusted-zone/trusted_working_data\")\n",
    "    .tableName(\"trusted_working_data\")\n",
    "    .addColumns(trusted_working_data_schema)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "refined_breathing_data_schema = StructType([\n",
    "    StructField('breathing_frq', ArrayType(DoubleType()), True),\n",
    "    StructField('breathing_psd', ArrayType(DoubleType()), True),\n",
    "    StructField('breathing_rate', LongType(), True),\n",
    "    StructField('breathing_signal', ArrayType(DoubleType()), True),\n",
    "    StructField('end_timestamp', TimestampType(), True),\n",
    "    StructField('start_timestamp', TimestampType(), True),\n",
    "    StructField('subject_uuid', StringType(), True),\n",
    "    StructField('video', StringType(), True),\n",
    "])\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://refined-zone/refined_breathing_data\")\n",
    "    .tableName(\"refined_breathing_data\")\n",
    "    .addColumns(refined_breathing_data_schema)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "refined_psd_frequencies_data_schema = StructType([\n",
    "    StructField(\"freq_ULF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"power_ULF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"freq_VLF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"power_VLF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"freq_LF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"power_LF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"freq_HF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"power_HF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"freq_VHF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"power_VHF\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"start_timestamp\", TimestampType(), True),\n",
    "    StructField(\"end_timestamp\", TimestampType(), True),\n",
    "    StructField(\"subject_uuid\", StringType(), True),\n",
    "    StructField(\"video\", StringType(), True)\n",
    "])\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://refined-zone/refined_psd_frequencies_data\")\n",
    "    .tableName(\"refined_psd_frequencies_data\")\n",
    "    .addColumns(refined_psd_frequencies_data_schema)\n",
    "    .execute()\n",
    ") \n",
    "\n",
    "\n",
    "refined_rri_histogram_data_schema = StructType([\n",
    "    StructField(\"bins\", ArrayType(DoubleType(), containsNull=True)),\n",
    "    StructField(\"counts\", ArrayType(LongType(), containsNull=True)),\n",
    "    StructField(\"end_timestamp\", TimestampType(), nullable=True),\n",
    "    StructField(\"rri\", ArrayType(DoubleType(), containsNull=True)),\n",
    "    StructField(\"start_timestamp\", TimestampType(), nullable=True),\n",
    "    StructField(\"subject_uuid\", StringType(), nullable=True),\n",
    "    StructField(\"video\", StringType(), nullable=True)\n",
    "])\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://refined-zone/refined_rri_histogram_data\")\n",
    "    .tableName(\"refined_rri_histogram_data\")\n",
    "    .addColumns(refined_rri_histogram_data_schema)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "\n",
    "refined_hr_data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"hr\", ArrayType(DoubleType())),\n",
    "        StructField(\"fps\", DoubleType()),\n",
    "        StructField(\"bpm\", LongType()),\n",
    "        StructField(\"start_timestamp\", TimestampType()),\n",
    "        StructField(\"end_timestamp\", TimestampType()),\n",
    "        StructField(\"subject_uuid\", StringType()),\n",
    "        StructField(\"video\", StringType()),\n",
    "    ]\n",
    ")\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://refined-zone/refined_hr_data\")\n",
    "    .tableName(\"refined_hr_data\")\n",
    "    .addColumns(refined_hr_data_schema)\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "trusted_sessions_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", LongType()),\n",
    "        StructField(\"start_timestamp\", TimestampType()),\n",
    "        StructField(\"end_timestamp\", TimestampType()),\n",
    "        StructField(\"subject_uuid\", StringType()),\n",
    "        StructField(\"video\", StringType()),\n",
    "    ]\n",
    ")\n",
    "(\n",
    "DeltaTable.createIfNotExists(spark)\n",
    "    .location(\"s3a://trusted-zone/trusted_sessions\")\n",
    "    .tableName(\"trusted_sessions\")\n",
    "    .addColumns(trusted_sessions_schema)\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that processed the video to extract BVP Signal using rPPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract BVP signal from a video\n",
    "def process_video(video_obj):\n",
    "    print(\"Processing: \", video_obj.object_name)\n",
    "\n",
    "    subject_uuid = str(video_obj.object_name).split(\"_\")[0]\n",
    "    tmp_video_path = \"/tmp/\" + str(video_obj.object_name)\n",
    "\n",
    "    client.fget_object(RAW_BUCKET_NAME, str(video_obj.object_name), tmp_video_path)\n",
    "\n",
    "    # TODO: Decrypt video first\n",
    "\n",
    "    filtered_bvp, fps, duration = video_extractor.extract_bvp_signal(tmp_video_path)\n",
    "\n",
    "    # Map all values from np.float64 to python float\n",
    "    filtered_bvp = list(map(float, filtered_bvp))\n",
    "\n",
    "    # Create and store data frame into a delta table\n",
    "    start_timestmap = datetime.now()\n",
    "    data = {\n",
    "        \"filtered_bvp\": filtered_bvp,\n",
    "        \"fps\": fps,\n",
    "        \"start_timestamp\": start_timestmap,\n",
    "        \"end_timestamp\": start_timestmap+timedelta(seconds=duration),\n",
    "        \"subject_uuid\": subject_uuid,\n",
    "        \"video\": video_obj.object_name,\n",
    "    }\n",
    "    filtered_bvp_df = spark.createDataFrame(data=[data])\n",
    "    filtered_bvp_df.write.format(\"delta\").mode(\"append\").save(\n",
    "        \"s3a://trusted-zone/trusted_filtered_bvp_signal\"\n",
    "    )\n",
    "\n",
    "    # Get the maximum ID from the table\n",
    "    session_df = spark.read.format(\"delta\").load(\"s3a://trusted-zone/trusted_sessions\")\n",
    "    if session_df.rdd.isEmpty():\n",
    "        max_id = -1\n",
    "    else:\n",
    "        max_id = session_df.select(max(col(\"id\"))).collect()[0][0]\n",
    "\n",
    "    # Add a new session row to the sessions table\n",
    "    session_data = {\n",
    "        \"id\": max_id + 1,\n",
    "        \"start_timestamp\": start_timestmap,\n",
    "        \"end_timestamp\": start_timestmap+timedelta(seconds=duration),\n",
    "        \"subject_uuid\": subject_uuid,\n",
    "        \"video\": video_obj.object_name,\n",
    "    }\n",
    "    session_data_df = spark.createDataFrame(data=[session_data])\n",
    "    session_data_df.write.format(\"delta\").mode(\"append\").save(\n",
    "        \"s3a://trusted-zone/trusted_sessions\"\n",
    "    )\n",
    "\n",
    "    subprocess.call([\"rm\", tmp_video_path])\n",
    "    print(\"Processing over\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting for a new BVP Signal\n",
    "### Then processes it and stores measures and working data into new delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bvp_signal_stream_changes_df = (\n",
    "    spark.readStream\n",
    "\t\t.format(\"delta\") \n",
    "    \t.option(\"readChangeFeed\", \"true\") \n",
    "\t    .option(\"startingVersion\", \"latest\")\n",
    "\t    .option(\"path\", \"s3a://trusted-zone/trusted_filtered_bvp_signal\")\n",
    "\t    .load()\n",
    ")\n",
    "\n",
    "def process_batch(df, epoch_id):\n",
    "\t# Collect all rows as a list of Rows\n",
    "\trows = df.collect()\n",
    "\n",
    "\t# Iterate over the list of Rows and do the processing\n",
    "\tfor row in rows:\n",
    "\t\t# Process extracted BVP signal (BPM, HRV ...)\n",
    "\t\ttmp_working_data, measures = signal_processer.process_bvp_signal(row[\"filtered_bvp\"], row[\"fps\"])\n",
    "\n",
    "\t\t# Change data types of working data, to be compatible with spark df\n",
    "\t\tworking_data = dict()\n",
    "\t\tfor key in list(tmp_working_data.keys()):\n",
    "\t\t\tif isinstance(tmp_working_data[key], np.ndarray):\n",
    "\t\t\t\tif isinstance(tmp_working_data[key][0], np.float):\n",
    "\t\t\t\t\tworking_data[key] = [float(v) for v in tmp_working_data[key]]\n",
    "\t\t\t\telif isinstance(tmp_working_data[key][0], np.int):\n",
    "\t\t\t\t\tworking_data[key] = [int(v) for v in tmp_working_data[key]]\n",
    "\t\t\telif isinstance(tmp_working_data[key], np.float):\n",
    "\t\t\t\tworking_data[key] = float(tmp_working_data[key])\n",
    "\n",
    "\n",
    "\t\t# CREATE MEASURES DATA FRAME\n",
    "\t\tmeasures[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\tmeasures[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\tmeasures[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\tmeasures[\"video\"] = row[\"video\"]\n",
    "\t\tmeasures_df = spark.createDataFrame(data=[measures])\n",
    "\t\t# --------------------\n",
    "\n",
    "\n",
    "\t\t# CREATE WORKING_DATA DATA FRAME\n",
    "\t\tworking_data[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\tworking_data[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\tworking_data[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\tworking_data[\"video\"] = row[\"video\"]\n",
    "\t\tworking_data_df = spark.createDataFrame(data=[working_data])\n",
    "\t\t# --------------------\n",
    "\n",
    "\n",
    "\t\t# CREATE BREATHING SIGNAL DATA FRAME\n",
    "\t\tbreathing_data = dict()\n",
    "\t\tbreathing_data[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\tbreathing_data[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\tbreathing_data[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\tbreathing_data[\"video\"] = row[\"video\"]\n",
    "\t\tbreathing_data[\"breathing_signal\"] = working_data[\"breathing_signal\"]\n",
    "\t\tbreathing_data[\"breathing_psd\"] = working_data[\"breathing_psd\"]\n",
    "\t\tbreathing_data[\"breathing_frq\"] = working_data[\"breathing_frq\"]\n",
    "\t\tbreathing_data[\"breathing_rate\"] = round(60 / (1 / measures[\"breathingrate\"])) #measures[\"breathingrate\"]\t\t\t\t# manter trusted ou refined? TODO:\n",
    "\t\t# TODO: Have the index as a column too. To build the plot with no transfomations needed?\t\t(datetime.datetime)\t\t\t????\n",
    "\t\tbreathing_data_df = spark.createDataFrame(data=[breathing_data])\n",
    "\t\t# --------------------\n",
    "\n",
    "\n",
    "\t\t# CREATE HR DATA FRAME\n",
    "\t\thr_data = dict()\n",
    "\t\thr_data[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\thr_data[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\thr_data[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\thr_data[\"video\"] = row[\"video\"]\n",
    "\t\thr_data[\"hr\"] = working_data[\"hr\"]\n",
    "\t\thr_data[\"fps\"] = row[\"fps\"]\n",
    "\t\thr_data[\"bpm\"] = round(measures[\"bpm\"]) #measures[\"bpm\"]\n",
    "\t\thr_data_df = spark.createDataFrame(data=[hr_data])\n",
    "\t\t# --------------------\n",
    "\n",
    "\n",
    "\t\t# CREATE RRI HISTOGRAM DATAFRAME\n",
    "\t\tn_bins = int(np.ceil(np.log2(len(working_data[\"RR_list\"]))) + 1)\n",
    "\t\tcounts, bins = np.histogram(working_data[\"RR_list\"], n_bins)\n",
    "\t\t\n",
    "\t\trri_histogram_data = dict()\n",
    "\t\trri_histogram_data[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\trri_histogram_data[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\trri_histogram_data[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\trri_histogram_data[\"video\"] = row[\"video\"]\n",
    "\t\trri_histogram_data[\"rri\"] = working_data[\"RR_list\"]\n",
    "\t\trri_histogram_data[\"counts\"] = [int(c) for c in counts]\n",
    "\t\trri_histogram_data[\"bins\"] = [float(b) for b in bins]\n",
    "\t\trri_histogram_data_df = spark.createDataFrame(data=[rri_histogram_data])\n",
    "\t\t# --------------------\n",
    "\n",
    "\n",
    "\t\t# CREATE PSD FREQUENCIES DATAFRAME\n",
    "\t\tfreq, power, frequency_band_index, labels = signal_processer.get_psd_frequencies(tmp_working_data[\"peaklist\"], tmp_working_data[\"RR_list\"], row[\"fps\"])\n",
    "\n",
    "\t\tpsd_frequencies_data = dict()\n",
    "\t\tfor band_index, label in zip(frequency_band_index, labels):\n",
    "\n",
    "\t\t\tfreq_key = \"freq_\"+str(label)\n",
    "\t\t\tpsd_frequencies_data[freq_key] = [float(v) for v in freq[band_index]]\n",
    "\n",
    "\t\t\tpower_key = \"power_\"+str(label)\n",
    "\t\t\tpsd_frequencies_data[power_key] = [float(v) for v in power[band_index]]\n",
    "\n",
    "\t\t\"\"\" psd_frequencies_data[\"freq\"] = [float(v) for v in data[\"freq\"]]\n",
    "\t\tpsd_frequencies_data[\"power\"] = [float(v) for v in data[\"power\"]]\n",
    "\t\tfor i in range(len(data[\"frequency_band_index\"])):\n",
    "\t\t\tpsd_frequencies_data[\"frequency_band_index\"] = [bool(v) for v in data[\"frequency_band_index\"][i]]\n",
    "\t\tpsd_frequencies_data[\"labels\"] = data[\"labels\"] \"\"\"\n",
    "\t\t#psd_frequencies_data[\"freq\"] = freq_list\n",
    "\t\t#psd_frequencies_data[\"power\"] = power_list\n",
    "\t\t#psd_frequencies_data[\"labels\"] = label_list\n",
    "\t\tpsd_frequencies_data[\"start_timestamp\"] = row[\"start_timestamp\"]\n",
    "\t\tpsd_frequencies_data[\"end_timestamp\"] = row[\"end_timestamp\"]\n",
    "\t\tpsd_frequencies_data[\"subject_uuid\"] = row[\"subject_uuid\"]\n",
    "\t\tpsd_frequencies_data[\"video\"] = row[\"video\"]\n",
    "\t\tpsd_frequencies_data_df = spark.createDataFrame(data=[psd_frequencies_data], schema=refined_psd_frequencies_data_schema)\n",
    "\t\t# --------------------\n",
    "\n",
    "\t\t# TODO: Create a data frame with the nk.hrv results? \n",
    "\n",
    "\t\t# Store results into a delta table\n",
    "\t\tmeasures_df.write.format(\"delta\").mode(\"append\").save(\"s3a://trusted-zone/trusted_heartpy_measures\")\n",
    "\t\tworking_data_df.write.format(\"delta\").mode(\"append\").save(\"s3a://trusted-zone/trusted_working_data\")\n",
    "\t\tbreathing_data_df.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/refined_breathing_data\")\n",
    "\t\thr_data_df.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/refined_hr_data\")\n",
    "\t\trri_histogram_data_df.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/refined_rri_histogram_data\")\n",
    "\t\tpsd_frequencies_data_df.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/refined_psd_frequencies_data\")\n",
    "\n",
    "filtered_bvp_signal_stream_changes_df.writeStream.foreachBatch(process_batch).start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listens for changes in \"rppg_hrv_parameters\" and prepares a new table for the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from pyspark.sql.functions import col, round\n",
    "\n",
    "measures_stream_changes_df = (\n",
    "    spark.readStream.format(\"delta\").option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", \"latest\")\n",
    "    .load(\"s3a://trusted-zone/heartpy_measures\")\n",
    ")\n",
    "\n",
    "\n",
    "def process_rppg_hrv_parameters(df, _):\n",
    "    df = (\n",
    "        df.withColumn(\n",
    "            \"breathingrate\", round(60 / (1 / col(\"breathingrate\"))).cast(\"Integer\")\n",
    "        )\n",
    "        .withColumn(\"bpm\", round(col(\"bpm\")).cast(\"Integer\"))\n",
    "        .select(\n",
    "            \"bpm\", \"breathingrate\", \"end_timestamp\", \"start_timestamp\", \"subject_uuid\"\t\t\t\t# TODO: Add Video column\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/heartpy_measures\")\n",
    "\n",
    "\n",
    "\n",
    "write_heartpy_measures_stream = (\n",
    "    measures_stream_changes_df.writeStream\n",
    "    .foreachBatch(process_rppg_hrv_parameters)\n",
    "    .start()\n",
    ") \"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Listens for changes in the BVP Signal table and in the Working Data table\n",
    "### Then joins the two changes of each table into one\n",
    "#### Used for HR Time Series Plot -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" filtered_bvp_signal_stream_changes_df = (\n",
    "    spark.readStream\n",
    "\t\t.format(\"delta\") \n",
    "    \t.option(\"readChangeFeed\", \"true\") \n",
    "\t    .option(\"startingVersion\", \"latest\")\n",
    "    \t.load(\"s3a://trusted-zone/filtered_bvp_signal\")\n",
    "        .withWatermark(\"start_timestamp\", \"10 minutes\")\n",
    ")\n",
    "\n",
    "\n",
    "working_data_stream_changes_df = (\n",
    "    spark.readStream\n",
    "\t\t.format(\"delta\") \n",
    "    \t.option(\"readChangeFeed\", \"true\") \n",
    "\t    .option(\"startingVersion\", \"latest\")\n",
    "    \t.load(\"s3a://trusted-zone/working_data\")\n",
    "\t\t.withWatermark(\"start_timestamp\", \"10 minutes\")\n",
    ")\n",
    "\n",
    "joined_stream = filtered_bvp_signal_stream_changes_df.join(working_data_stream_changes_df, [\"start_timestamp\", \"end_timestamp\", \"subject_uuid\", \"video\"])\n",
    "\n",
    "def write_to_hr_data(df, _):\n",
    "\tdf.show()\n",
    "\tdf.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/hr_data\")\n",
    "\n",
    "\n",
    "write_joined_stream = joined_stream.select(\"hr\", \"fps\", \"start_timestamp\", \"end_timestamp\", \"subject_uuid\", \"video\").writeStream.foreachBatch(write_to_hr_data).start()\n",
    "\n",
    "\n",
    "working_data_stream_changes_df = (\n",
    "    spark.readStream\n",
    "\t\t.format(\"delta\") \n",
    "    \t.option(\"readChangeFeed\", \"true\") \n",
    "\t    .option(\"startingVersion\", \"latest\")\n",
    "    \t.load(\"s3a://trusted-zone/working_data\")\n",
    "\t\t.withWatermark(\"start_timestamp\", \"10 minutes\")\n",
    ") \"\"\"\n",
    "\n",
    "\"\"\" def write_to_hr_data(df, _):\n",
    "\tdf.show()\n",
    "\n",
    "\t# Maybe:  (fazer distribuição dos hr por timestamps)\n",
    "\t\n",
    "\t#\tduration = end_timestamp - start_timestamp\n",
    "\t#\ttime_interval = duration / len(hr_list)\n",
    "\n",
    "\n",
    "\tdf.write.format(\"delta\").mode(\"append\").save(\"s3a://refined-zone/hr_data\")\n",
    "\n",
    "working_data_stream_changes_df.select(\"hr\", \"start_timestamp\", \"end_timestamp\", \"subject_uuid\", \"video\").writeStream.foreachBatch(write_to_hr_data).start() \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Loop waiting for new files (videos) to arrive at the raw bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" already_processed = []\n",
    "objects = client.list_objects(RAW_BUCKET_NAME)\n",
    "for obj in objects:\n",
    "    already_processed.append(obj.etag) \"\"\"      \n",
    "    \n",
    "    #TODO: REMOVE\n",
    "\n",
    "try:\n",
    "    most_recent_object = None\n",
    "    most_recent_time = None\n",
    "    while True:\n",
    "        # List all objects with prefix\n",
    "        objects = client.list_objects(RAW_BUCKET_NAME)\n",
    "\n",
    "        # Get the most recent object                                    # TODO: Problem if two videos are added why most recent?\n",
    "        for obj in objects:\n",
    "            if most_recent_time is None or obj.last_modified > most_recent_time:\n",
    "                most_recent_object = obj\n",
    "                most_recent_time = obj.last_modified\n",
    "\n",
    "        if most_recent_object is not None:\n",
    "            file_extension = str(obj.object_name).split(\".\")[-1]\n",
    "\n",
    "            if file_extension in [\"avi\", \"mov\", \"mp4\"] and obj.etag not in already_processed:\n",
    "                process_video(most_recent_object)\n",
    "                #print(\"process\", most_recent_object.object_name)\n",
    "                most_recent_object = None\n",
    "            \n",
    "        # Wait for new events\n",
    "        time.sleep(5)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop watching on keyboard interrupt\n",
    "    spark.stop()\n",
    "    pass \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_obj(obj):\n",
    "    print(f\"New file: {obj.object_name}\")\n",
    "\n",
    "    # TODO: Encrypt file (cant be done with snakebite, because the file contents cannot be read)\n",
    "    # ...\n",
    "\n",
    "    subject_uuid, extension = str(obj.object_name).split(\".\")\n",
    "    filename_date = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    new_filename = subject_uuid+\"_\"+filename_date+\".\"+extension\n",
    "\n",
    "    client.copy_object(\n",
    "        RAW_BUCKET_NAME,\n",
    "        new_filename,\n",
    "        CopySource(LANDING_BUCKET_NAME, str(obj.object_name)),\n",
    "    ) \n",
    "    client.remove_object(LANDING_BUCKET_NAME, str(obj.object_name))\n",
    "    print(f\"File processed: {obj.object_name}\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # List all objects\n",
    "        objects = client.list_objects(LANDING_BUCKET_NAME)\n",
    "\n",
    "        for obj in objects:\n",
    "            file_extension = str(obj.object_name).split(\".\")[-1]\n",
    "\n",
    "            if file_extension in [\"avi\", \"mov\", \"mp4\"]:\n",
    "                process_new_obj(obj)\n",
    "                process_video(obj)\n",
    "            \n",
    "        # Wait for new events\n",
    "        time.sleep(5)\n",
    "except KeyboardInterrupt:\n",
    "    # Stop watching on keyboard interrupt\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://refined-zone/refined_hr_data\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://trusted-zone/trusted_working_data\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://refined-zone/refined_breathing_data\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://refined-zone/refined_psd_frequencies_data\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://refined-zone/refined_rri_histogram_data\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://trusted-zone/trusted_sessions\")\n",
    "df = delta_table.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   string|\n",
      "+---------+\n",
      "|test_text|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "delta_table = DeltaTable.forPath(spark, \"s3a://refined-zone/refined_hr_data\")\n",
    "df = delta_table.toDF()\n",
    "\n",
    "# TODO: EXAMPLE OF DATA QUERY WITH SPARK SQL AND DATAFRAME\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta-lake-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0328d284aec4e0c2b3a59ce7e7bdb37f94fa4c0d2410f63ed67021b90874bfff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
